# è¯„ä¼°

ä½¿ç”¨å…¨é¢çš„æŒ‡æ ‡ã€æ•°æ®é›†ã€LLM è¯„åˆ¤ã€A/B æµ‹è¯•å’ŒæŒç»­è¯„ä¼°ç®¡é“æ¥æµ‹è¯•å’Œè¯„ä¼°æ‚¨çš„æ™ºèƒ½ä½“ã€‚

## æ¦‚è¿°

è¯„ä¼°å¯¹äºæ„å»ºå¯é çš„æ™ºèƒ½ä½“è‡³å…³é‡è¦ã€‚æœ¬æŒ‡å—æ¶µç›–äº†ä»å•å…ƒæµ‹è¯•åˆ°ç”Ÿäº§ç›‘æ§çš„ç³»ç»ŸåŒ–æ–¹æ³•æ¥è¡¡é‡æ™ºèƒ½ä½“æ€§èƒ½ã€‚

**æ‚¨å°†å­¦åˆ°ï¼š**
- è¯„ä¼°æŒ‡æ ‡å’Œæ¡†æ¶
- æ•°æ®é›†åˆ›å»ºå’Œç®¡ç†
- LLM ä½œä¸ºè¯„åˆ¤çš„æ¨¡å¼
- A/B æµ‹è¯•ç­–ç•¥
- æŒç»­è¯„ä¼°
- å›å½’æ£€æµ‹

---

## è¯„ä¼°æ¡†æ¶

### è¯„ä¼°ç»´åº¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                         â”‚
â”‚  Correctness  â”‚  å‡†ç¡®åº¦å¦‚ä½•ï¼Ÿ            â”‚
â”‚  Helpfulness  â”‚  æ˜¯å¦è§£å†³äº†ä»»åŠ¡ï¼Ÿ        â”‚
â”‚  Safety       â”‚  æ˜¯å¦æœ‰æœ‰å®³è¾“å‡ºï¼Ÿ        â”‚
â”‚  Performance  â”‚  é€Ÿåº¦ã€æˆæœ¬ã€ä»¤ç‰Œæ•°      â”‚
â”‚  Reliability  â”‚  ç»“æœæ˜¯å¦ä¸€è‡´ï¼Ÿ          â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### åŸºç¡€è¯„ä¼°

```typescript
import { evaluate } from '@seashore/platform'

const results = await evaluate({
  agent,
  testCases: [
    {
      input: 'What is the capital of France?',
      expectedOutput: 'Paris',
    },
    {
      input: 'Calculate 15 * 24',
      expectedOutput: '360',
    },
  ],
  metrics: ['accuracy', 'latency', 'cost'],
})

console.table(results.summary)
```

---

## æŒ‡æ ‡

### æ­£ç¡®æ€§æŒ‡æ ‡

**ç²¾ç¡®åŒ¹é…ï¼š**
```typescript
function exactMatch(predicted: string, expected: string): number {
  return predicted.trim().toLowerCase() === expected.trim().toLowerCase() 
    ? 1 
    : 0
}
```

**æ¨¡ç³ŠåŒ¹é…ï¼š**
```typescript
import { similarity } from 'string-similarity'

function fuzzyMatch(predicted: string, expected: string, threshold = 0.8): number {
  const score = similarity(predicted, expected)
  return score >= threshold ? 1 : 0
}
```

**åŒ…å«ï¼š**
```typescript
function contains(predicted: string, expected: string): number {
  return predicted.toLowerCase().includes(expected.toLowerCase()) 
    ? 1 
    : 0
}
```

**è¯­ä¹‰ç›¸ä¼¼åº¦ï¼š**
```typescript
import { createEmbeddingAdapter } from '@seashore/core'

const embeddings = createEmbeddingAdapter({
  provider: 'openai',
  model: 'text-embedding-3-small',
  apiKey: process.env.OPENAI_API_KEY!,
})

async function semanticSimilarity(
  predicted: string,
  expected: string
): Promise<number> {
  const [predEmbedding, expEmbedding] = await embeddings.embedMany([
    predicted,
    expected,
  ])
  
  return cosineSimilarity(predEmbedding, expEmbedding)
}

function cosineSimilarity(a: number[], b: number[]): number {
  const dot = a.reduce((sum, val, i) => sum + val * b[i], 0)
  const normA = Math.sqrt(a.reduce((sum, val) => sum + val ** 2, 0))
  const normB = Math.sqrt(b.reduce((sum, val) => sum + val ** 2, 0))
  return dot / (normA * normB)
}
```

### æ€§èƒ½æŒ‡æ ‡

```typescript
interface PerformanceMetrics {
  latency: number // ms
  tokenUsage: {
    input: number
    output: number
    total: number
  }
  cost: number // USD
  throughput: number // requests/second
}

function measurePerformance(
  startTime: number,
  endTime: number,
  tokenUsage: { input: number; output: number }
): PerformanceMetrics {
  const latency = endTime - startTime
  
  // æˆæœ¬è®¡ç®—ï¼ˆGPT-4 ç¤ºä¾‹ï¼‰
  const inputCostPer1M = 30 // æ¯ 100 ä¸‡è¾“å…¥ä»¤ç‰Œ $30
  const outputCostPer1M = 60 // æ¯ 100 ä¸‡è¾“å‡ºä»¤ç‰Œ $60
  
  const cost =
    (tokenUsage.input / 1_000_000) * inputCostPer1M +
    (tokenUsage.output / 1_000_000) * outputCostPer1M
  
  return {
    latency,
    tokenUsage: {
      ...tokenUsage,
      total: tokenUsage.input + tokenUsage.output,
    },
    cost,
    throughput: 1000 / latency, // æ¯ç§’è¯·æ±‚æ•°
  }
}
```

### è´¨é‡æŒ‡æ ‡

```typescript
interface QualityMetrics {
  coherence: number // 0-1
  relevance: number // 0-1
  completeness: number // 0-1
  fluency: number // 0-1
}

async function assessQuality(
  response: string,
  query: string,
  context: string[]
): Promise<QualityMetrics> {
  // ä½¿ç”¨ LLM ä½œä¸ºè¯„åˆ¤
  const assessment = await llm('gpt-4o').chat([
    {
      role: 'system',
      content: `Rate the response on these dimensions (0-1):
- Coherence: Is it logical and well-structured?
- Relevance: Does it address the query?
- Completeness: Does it fully answer the question?
- Fluency: Is it well-written?

Respond in JSON format.`,
    },
    {
      role: 'user',
      content: `Query: ${query}\n\nContext: ${context.join('\n')}\n\nResponse: ${response}`,
    },
  ])
  
  return JSON.parse(assessment.content)
}
```

---

## æµ‹è¯•æ•°æ®é›†

### æ•°æ®é›†ç»“æ„

```typescript
interface TestCase {
  id: string
  input: {
    query: string
    context?: Record<string, any>
  }
  expectedOutput: {
    content?: string
    toolCalls?: string[]
    metadata?: Record<string, any>
  }
  category: string
  difficulty: 'easy' | 'medium' | 'hard'
}

const testDataset: TestCase[] = [
  {
    id: 'qa-001',
    input: {
      query: 'What is the capital of France?',
    },
    expectedOutput: {
      content: 'Paris',
    },
    category: 'factual',
    difficulty: 'easy',
  },
  {
    id: 'tool-001',
    input: {
      query: 'Search for the latest news about AI',
    },
    expectedOutput: {
      toolCalls: ['search_web'],
    },
    category: 'tool-use',
    difficulty: 'medium',
  },
  {
    id: 'reasoning-001',
    input: {
      query: 'If a train leaves NYC at 2pm traveling 60mph, and another leaves Chicago at 3pm traveling 70mph, when do they meet?',
    },
    expectedOutput: {
      toolCalls: ['calculator'],
    },
    category: 'reasoning',
    difficulty: 'hard',
  },
]
```

### æ•°æ®é›†ç”Ÿæˆ

```typescript
async function generateTestCases(
  domain: string,
  count: number
): Promise<TestCase[]> {
  const cases: TestCase[] = []
  
  for (let i = 0; i < count; i++) {
    const generated = await llm('gpt-4o').chat([
      {
        role: 'system',
        content: `Generate a test case for a ${domain} agent.
Include:
- A realistic user query
- Expected output or behavior
- Category and difficulty

Respond in JSON format.`,
      },
      {
        role: 'user',
        content: `Generate test case ${i + 1} of ${count}`,
      },
    ])
    
    const testCase = JSON.parse(generated.content)
    cases.push({
      id: `gen-${i + 1}`,
      ...testCase,
    })
  }
  
  return cases
}
```

### æ•°æ®é›†ç®¡ç†

```typescript
class TestDatasetManager {
  private dataset: TestCase[] = []
  
  async load(path: string): Promise<void> {
    const data = await fs.readFile(path, 'utf-8')
    this.dataset = JSON.parse(data)
  }
  
  async save(path: string): Promise<void> {
    await fs.writeFile(
      path,
      JSON.stringify(this.dataset, null, 2),
      'utf-8'
    )
  }
  
  add(testCase: TestCase): void {
    this.dataset.push(testCase)
  }
  
  filter(criteria: Partial<TestCase>): TestCase[] {
    return this.dataset.filter(tc => {
      return Object.entries(criteria).every(
        ([key, value]) => tc[key] === value
      )
    })
  }
  
  sample(count: number, category?: string): TestCase[] {
    let pool = category
      ? this.filter({ category } as any)
      : this.dataset
    
    return pool.sort(() => Math.random() - 0.5).slice(0, count)
  }
  
  getStats() {
    return {
      total: this.dataset.length,
      byCategory: this.groupBy('category'),
      byDifficulty: this.groupBy('difficulty'),
    }
  }
  
  private groupBy(key: keyof TestCase) {
    return this.dataset.reduce((acc, tc) => {
      const value = tc[key] as string
      acc[value] = (acc[value] || 0) + 1
      return acc
    }, {} as Record<string, number>)
  }
}
```

---

## LLM ä½œä¸ºè¯„åˆ¤

### åŸºç¡€è¯„åˆ¤

```typescript
async function llmJudge(
  query: string,
  response: string,
  criteria: string
): Promise<{ score: number; reasoning: string }> {
  const judgment = await llm('gpt-4o').chat([
    {
      role: 'system',
      content: `You are an expert evaluator. Rate the response on a scale of 1-10 based on: ${criteria}

Provide your rating and reasoning in JSON format:
{
  "score": <1-10>,
  "reasoning": "<explanation>"
}`,
    },
    {
      role: 'user',
      content: `Query: ${query}\n\nResponse: ${response}`,
    },
  ])
  
  return JSON.parse(judgment.content)
}
```

### å¤šæ ‡å‡†è¯„åˆ¤

```typescript
interface JudgmentCriteria {
  name: string
  description: string
  weight: number
}

async function multiCriteriaJudge(
  query: string,
  response: string,
  criteria: JudgmentCriteria[]
): Promise<{ overallScore: number; breakdown: Record<string, any> }> {
  const assessments = await Promise.all(
    criteria.map(async (criterion) => {
      const result = await llmJudge(query, response, criterion.description)
      return {
        criterion: criterion.name,
        score: result.score,
        reasoning: result.reasoning,
        weight: criterion.weight,
      }
    })
  )
  
  const weightedScore = assessments.reduce(
    (sum, a) => sum + (a.score / 10) * a.weight,
    0
  )
  
  const totalWeight = criteria.reduce((sum, c) => sum + c.weight, 0)
  
  return {
    overallScore: weightedScore / totalWeight,
    breakdown: assessments,
  }
}

// ç”¨æ³•
const criteria: JudgmentCriteria[] = [
  {
    name: 'accuracy',
    description: 'Is the information factually correct?',
    weight: 0.4,
  },
  {
    name: 'helpfulness',
    description: 'Does it fully address the user query?',
    weight: 0.3,
  },
  {
    name: 'clarity',
    description: 'Is it clear and easy to understand?',
    weight: 0.3,
  },
]

const result = await multiCriteriaJudge(query, response, criteria)
```

### æˆå¯¹æ¯”è¾ƒ

```typescript
async function pairwiseCompare(
  query: string,
  responseA: string,
  responseB: string
): Promise<'A' | 'B' | 'tie'> {
  const comparison = await llm('gpt-4o').chat([
    {
      role: 'system',
      content: `Compare two responses to the same query. Which is better overall?
Respond with: A, B, or tie`,
    },
    {
      role: 'user',
      content: `Query: ${query}\n\nResponse A: ${responseA}\n\nResponse B: ${responseB}`,
    },
  ])
  
  const result = comparison.content.trim().toUpperCase()
  return result === 'TIE' ? 'tie' : (result as 'A' | 'B')
}
```

---

## è¿è¡Œè¯„ä¼°

### åŸºç¡€è¯„ä¼°å¾ªç¯

```typescript
async function evaluateAgent(
  agent: ReActAgent,
  testCases: TestCase[]
): Promise<EvaluationResults> {
  const results: TestResult[] = []
  
  for (const testCase of testCases) {
    const startTime = Date.now()
    
    try {
      const response = await agent.run([
        { role: 'user', content: testCase.input.query },
      ])
      
      const endTime = Date.now()
      
      // æµ‹é‡æŒ‡æ ‡
      const accuracy = await semanticSimilarity(
        response.result.content,
        testCase.expectedOutput.content || ''
      )
      
      const quality = await assessQuality(
        response.result.content,
        testCase.input.query,
        []
      )
      
      results.push({
        testCaseId: testCase.id,
        passed: accuracy > 0.8,
        metrics: {
          accuracy,
          latency: endTime - startTime,
          ...quality,
        },
        output: response.result.content,
      })
    } catch (error) {
      results.push({
        testCaseId: testCase.id,
        passed: false,
        error: error instanceof Error ? error.message : String(error),
      })
    }
  }
  
  return aggregateResults(results)
}

function aggregateResults(results: TestResult[]): EvaluationResults {
  const passed = results.filter(r => r.passed).length
  
  return {
    totalTests: results.length,
    passed,
    failed: results.length - passed,
    passRate: passed / results.length,
    averageLatency: average(results.map(r => r.metrics?.latency || 0)),
    results,
  }
}
```

### å¹¶è¡Œè¯„ä¼°

```typescript
async function evaluateParallel(
  agent: ReActAgent,
  testCases: TestCase[],
  concurrency = 5
): Promise<EvaluationResults> {
  const results: TestResult[] = []
  
  // æ‰¹é‡å¤„ç†
  for (let i = 0; i < testCases.length; i += concurrency) {
    const batch = testCases.slice(i, i + concurrency)
    
    const batchResults = await Promise.all(
      batch.map(testCase => evaluateSingleCase(agent, testCase))
    )
    
    results.push(...batchResults)
    
    console.log(`Progress: ${results.length}/${testCases.length}`)
  }
  
  return aggregateResults(results)
}
```

---

## A/B æµ‹è¯•

### å®éªŒè®¾ç½®

```typescript
interface Experiment {
  id: string
  variants: {
    name: string
    agent: ReActAgent
    weight: number
  }[]
  metrics: string[]
  duration: number // å°æ—¶
}

class ABTester {
  private results = new Map<string, TestResult[]>()
  
  async run(experiment: Experiment, testCases: TestCase[]): Promise<ABTestResults> {
    console.log(`Starting A/B test: ${experiment.id}`)
    
    for (const variant of experiment.variants) {
      console.log(`Testing variant: ${variant.name}`)
      
      const results = await evaluateAgent(variant.agent, testCases)
      this.results.set(variant.name, results.results)
    }
    
    return this.analyze(experiment)
  }
  
  private analyze(experiment: Experiment): ABTestResults {
    const variantStats = experiment.variants.map(variant => {
      const results = this.results.get(variant.name)!
      
      return {
        variant: variant.name,
        passRate: results.filter(r => r.passed).length / results.length,
        avgLatency: average(results.map(r => r.metrics?.latency || 0)),
        avgAccuracy: average(results.map(r => r.metrics?.accuracy || 0)),
      }
    })
    
    // ç»Ÿè®¡æ˜¾è‘—æ€§
    const baseline = variantStats[0]
    const comparisons = variantStats.slice(1).map(variant => ({
      variant: variant.name,
      passRateDelta: variant.passRate - baseline.passRate,
      latencyDelta: variant.avgLatency - baseline.avgLatency,
      significant: this.isSignificant(baseline, variant),
    }))
    
    return {
      experimentId: experiment.id,
      variantStats,
      comparisons,
      winner: this.selectWinner(variantStats),
    }
  }
  
  private isSignificant(baseline: any, variant: any): boolean {
    // ç®€åŒ–çš„æ˜¾è‘—æ€§æµ‹è¯•
    const delta = Math.abs(variant.passRate - baseline.passRate)
    return delta > 0.05 // 5% æ”¹è¿›é˜ˆå€¼
  }
  
  private selectWinner(stats: any[]): string {
    return stats.reduce((best, current) =>
      current.passRate > best.passRate ? current : best
    ).variant
  }
}
```

### è¿è¡Œ A/B æµ‹è¯•

```typescript
// è®¾ç½®å˜ä½“
const baselineAgent = createReActAgent({
  model: () => llm('gpt-4o'),
  systemPrompt: 'You are helpful.',
  tools: [searchTool],
})

const experimentalAgent = createReActAgent({
  model: () => llm('gpt-4o'),
  systemPrompt: 'You are helpful. Always verify facts before responding.',
  tools: [searchTool, factCheckTool],
  maxIterations: 15,
})

// è¿è¡Œå®éªŒ
const tester = new ABTester()
const results = await tester.run(
  {
    id: 'fact-checking-experiment',
    variants: [
      { name: 'baseline', agent: baselineAgent, weight: 0.5 },
      { name: 'with-fact-check', agent: experimentalAgent, weight: 0.5 },
    ],
    metrics: ['accuracy', 'latency', 'cost'],
    duration: 24,
  },
  testDataset
)

console.table(results.variantStats)
console.log('Winner:', results.winner)
```

---

## æŒç»­è¯„ä¼°

### ç›‘æ§ç®¡é“

```typescript
class ContinuousEvaluator {
  private testSuite: TestCase[]
  private schedule: NodeJS.Timer | null = null
  
  constructor(testSuite: TestCase[]) {
    this.testSuite = testSuite
  }
  
  start(intervalMs: number, agent: ReActAgent): void {
    console.log('Starting continuous evaluation...')
    
    this.schedule = setInterval(async () => {
      try {
        const results = await evaluateAgent(agent, this.testSuite)
        
        // è®°å½•ç»“æœ
        await this.logResults(results)
        
        // æ£€æŸ¥å›å½’
        const regressions = await this.detectRegressions(results)
        
        if (regressions.length > 0) {
          await this.alertRegression(regressions)
        }
      } catch (error) {
        console.error('Evaluation failed:', error)
      }
    }, intervalMs)
  }
  
  stop(): void {
    if (this.schedule) {
      clearInterval(this.schedule)
      this.schedule = null
    }
  }
  
  private async logResults(results: EvaluationResults): Promise<void> {
    await db.evaluations.create({
      data: {
        timestamp: new Date(),
        passRate: results.passRate,
        averageLatency: results.averageLatency,
        results: results.results,
      },
    })
  }
  
  private async detectRegressions(
    current: EvaluationResults
  ): Promise<Regression[]> {
    // è·å–å†å²åŸºå‡†
    const baseline = await db.evaluations.findFirst({
      where: {
        timestamp: { gte: new Date(Date.now() - 7 * 24 * 60 * 60 * 1000) },
      },
      orderBy: { passRate: 'desc' },
    })
    
    if (!baseline) return []
    
    const regressions: Regression[] = []
    
    // æ£€æŸ¥é€šè¿‡ç‡å›å½’
    if (current.passRate < baseline.passRate - 0.1) {
      regressions.push({
        metric: 'passRate',
        current: current.passRate,
        baseline: baseline.passRate,
        delta: current.passRate - baseline.passRate,
      })
    }
    
    // æ£€æŸ¥å»¶è¿Ÿå›å½’
    if (current.averageLatency > baseline.averageLatency * 1.5) {
      regressions.push({
        metric: 'latency',
        current: current.averageLatency,
        baseline: baseline.averageLatency,
        delta: current.averageLatency - baseline.averageLatency,
      })
    }
    
    return regressions
  }
  
  private async alertRegression(regressions: Regression[]): Promise<void> {
    console.error('ğŸš¨ Regressions detected:', regressions)
    
    // å‘é€è­¦æŠ¥ï¼ˆç”µå­é‚®ä»¶ã€Slack ç­‰ï¼‰
    await notificationService.send({
      title: 'Agent Performance Regression',
      message: `Detected ${regressions.length} performance regressions`,
      details: regressions,
    })
  }
}

// ç”¨æ³•
const evaluator = new ContinuousEvaluator(testDataset)
evaluator.start(60 * 60 * 1000, agent) // æ¯å°æ—¶è¿è¡Œä¸€æ¬¡
```

---

## æœ€ä½³å®è·µ

### æµ‹è¯•è¦†ç›–
- [ ] è¦†ç›–å¸¸è§ç”¨ä¾‹ï¼ˆ80%ï¼‰
- [ ] åŒ…æ‹¬è¾¹ç¼˜æƒ…å†µï¼ˆ15%ï¼‰
- [ ] æµ‹è¯•å¤±è´¥æ¨¡å¼ï¼ˆ5%ï¼‰
- [ ] å¹³è¡¡éš¾åº¦

### æŒ‡æ ‡
- [ ] ä½¿ç”¨å¤šä¸ªæŒ‡æ ‡
- [ ] æµ‹é‡å¯¹ç”¨æˆ·é‡è¦çš„å†…å®¹
- [ ] éšæ—¶é—´è·Ÿè¸ª
- [ ] è®¾ç½®è­¦æŠ¥é˜ˆå€¼

### LLM è¯„åˆ¤
- [ ] ä½¿ç”¨å¼ºå¤§çš„æ¨¡å‹ï¼ˆGPT-4+ï¼‰
- [ ] æä¾›æ˜ç¡®çš„æ ‡å‡†
- [ ] ä½¿ç”¨äººå·¥æ ‡ç­¾éªŒè¯è¯„åˆ¤
- [ ] å¯¹é‡è¦æµ‹è¯•ä½¿ç”¨å¤šä¸ªè¯„åˆ¤

### æŒç»­è¯„ä¼°
- [ ] å®šæœŸè¿è¡Œï¼ˆæ¯å°æ—¶/æ¯å¤©ï¼‰
- [ ] ç›‘æ§è¶‹åŠ¿
- [ ] å›å½’æ—¶å‘å‡ºè­¦æŠ¥
- [ ] ç‰ˆæœ¬åŒ–æµ‹è¯•æ•°æ®é›†

---

## ä¸‹ä¸€æ­¥

- **[æµ‹è¯•æŒ‡å—](./testing.md)** - å•å…ƒå’Œé›†æˆæµ‹è¯•
- **[æ€§èƒ½æŒ‡å—](./performance.md)** - ä¼˜åŒ–æ™ºèƒ½ä½“æ€§èƒ½
- **[æ„å»ºæ™ºèƒ½ä½“](./building-agents.md)** - åº”ç”¨è¯„ä¼°è§è§£

---

## å…¶ä»–èµ„æº

- **[API å‚è€ƒ](/docs/api/platform.md#evaluation)** - è¯„ä¼° API
- **[ç¤ºä¾‹](/examples/)** - è¯„ä¼°ç¤ºä¾‹
- **[æœ€ä½³å®è·µ](/docs/best-practices)** - æ›´å¤šæŒ‡å—
